{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "from e3nn import o3\n",
    "from e3nn.util import jit\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from mace import data, modules, tools\n",
    "from mace.tools import torch_geometric\n",
    "torch.set_default_dtype(torch.float64)\n",
    "config = data.Configuration(\n",
    "    atomic_numbers=np.array([8, 4, 1]),\n",
    "    positions=np.array(\n",
    "        [\n",
    "            [0.0, -2.0, 0.0],\n",
    "            [1.0, 0.0, 0.0],\n",
    "            [0.0, 1.0, 0.0],\n",
    "        ]\n",
    "    ),\n",
    "    forces=np.array(\n",
    "        [\n",
    "            [0.0, -1.3, 0.0],\n",
    "            [1.0, 0.2, 0.0],\n",
    "            [0.0, 1.1, 0.3],\n",
    "        ]\n",
    "    ),\n",
    "    energy=-1.5,\n",
    "    charges=np.array([-2.0, 1.0, 1.0]),\n",
    "    dipole=np.array([-1.5, 1.5, 2.0]),\n",
    ")\n",
    "# Created the rotated environment\n",
    "rot = R.from_euler(\"z\", 60, degrees=True).as_matrix()\n",
    "positions_rotated = np.array(rot @ config.positions.T).T\n",
    "config_rotated = data.Configuration(\n",
    "    atomic_numbers=np.array([8, 4, 1]),\n",
    "    positions=positions_rotated,\n",
    "    forces=np.array(\n",
    "        [\n",
    "            [0.0, -1.3, 0.0],\n",
    "            [1.0, 0.2, 0.0],\n",
    "            [0.0, 1.1, 0.3],\n",
    "        ]\n",
    "    ),\n",
    "    energy=-1.5,\n",
    "    charges=np.array([-2.0, 1.0, 1.0]),\n",
    "    dipole=np.array([-1.5, 1.5, 2.0]),\n",
    ")\n",
    "table = tools.AtomicNumberTable([1, 4, 8])\n",
    "atomic_energies = np.array([1.0, 3.0, 4.0], dtype=float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/Users/sanggyu/miniconda3/envs/mace/lib/python3.12/site-packages/torch/jit/_check.py:177: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_config = dict(\n",
    "    r_max=5,\n",
    "    num_bessel=8,\n",
    "    num_polynomial_cutoff=6,\n",
    "    max_ell=2,\n",
    "    interaction_cls=modules.interaction_classes[\n",
    "        \"RealAgnosticResidualInteractionBlock\"\n",
    "    ],\n",
    "    interaction_cls_first=modules.interaction_classes[\n",
    "        \"RealAgnosticResidualInteractionBlock\"\n",
    "    ],\n",
    "    num_interactions=5,\n",
    "    num_elements=3,\n",
    "    hidden_irreps=o3.Irreps(\"32x0e + 32x1o\"),\n",
    "    MLP_irreps=o3.Irreps(\"16x0e\"),\n",
    "    gate=torch.nn.functional.silu,\n",
    "    atomic_energies=atomic_energies,\n",
    "    avg_num_neighbors=8,\n",
    "    atomic_numbers=table.zs,\n",
    "    correlation=3,\n",
    "    radial_type=\"bessel\",\n",
    ")\n",
    "model = modules.MACE(**model_config)\n",
    "model_compiled = jit.compile(model)\n",
    "atomic_data = data.AtomicData.from_config(config, z_table=table, cutoff=3.0)\n",
    "atomic_data2 = data.AtomicData.from_config(\n",
    "    config_rotated, z_table=table, cutoff=3.0\n",
    ")\n",
    "data_loader = torch_geometric.dataloader.DataLoader(\n",
    "    dataset=[atomic_data, atomic_data2],\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "batch = next(iter(data_loader))\n",
    "output1 = model(batch.to_dict(), training=True)\n",
    "output2 = model_compiled(batch.to_dict(), training=True)\n",
    "assert torch.allclose(output1[\"energy\"][0], output2[\"energy\"][0])\n",
    "assert torch.allclose(output2[\"energy\"][0], output2[\"energy\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=LinearReadoutBlock\n",
      "  (linear): RecursiveScriptModule(\n",
      "    original_name=Linear\n",
      "    (_compiled_main): RecursiveScriptModule(original_name=GraphModule)\n",
      "  )\n",
      ")\n",
      "RecursiveScriptModule(\n",
      "  original_name=LinearReadoutBlock\n",
      "  (linear): RecursiveScriptModule(\n",
      "    original_name=Linear\n",
      "    (_compiled_main): RecursiveScriptModule(original_name=GraphModule)\n",
      "  )\n",
      ")\n",
      "RecursiveScriptModule(\n",
      "  original_name=LinearReadoutBlock\n",
      "  (linear): RecursiveScriptModule(\n",
      "    original_name=Linear\n",
      "    (_compiled_main): RecursiveScriptModule(original_name=GraphModule)\n",
      "  )\n",
      ")\n",
      "RecursiveScriptModule(\n",
      "  original_name=LinearReadoutBlock\n",
      "  (linear): RecursiveScriptModule(\n",
      "    original_name=Linear\n",
      "    (_compiled_main): RecursiveScriptModule(original_name=GraphModule)\n",
      "  )\n",
      ")\n",
      "RecursiveScriptModule(\n",
      "  original_name=NonLinearReadoutBlock\n",
      "  (linear_1): RecursiveScriptModule(\n",
      "    original_name=Linear\n",
      "    (_compiled_main): RecursiveScriptModule(original_name=GraphModule)\n",
      "  )\n",
      "  (non_linearity): RecursiveScriptModule(\n",
      "    original_name=Activation\n",
      "    (acts): RecursiveScriptModule(\n",
      "      original_name=ModuleList\n",
      "      (0): RecursiveScriptModule(original_name=normalize2mom)\n",
      "    )\n",
      "  )\n",
      "  (linear_2): RecursiveScriptModule(\n",
      "    original_name=Linear\n",
      "    (_compiled_main): RecursiveScriptModule(original_name=GraphModule)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'energy': tensor([8.1135, 8.1135], grad_fn=<SumBackward1>),\n",
       " 'node_energy': tensor([3.7032, 2.8552, 1.5551, 3.7032, 2.8552, 1.5551],\n",
       "        grad_fn=<SumBackward1>),\n",
       " 'contributions': tensor([[ 8.0000,  0.1263,  0.1325, -0.0208, -0.0144, -0.1101],\n",
       "         [ 8.0000,  0.1263,  0.1325, -0.0208, -0.0144, -0.1101]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " 'forces': tensor([[-0.0159,  0.0239, -0.0000],\n",
       "         [-0.2382, -0.0917, -0.0000],\n",
       "         [ 0.2541,  0.0679, -0.0000],\n",
       "         [ 0.0127,  0.0257, -0.0000],\n",
       "         [-0.1985,  0.1604, -0.0000],\n",
       "         [ 0.1858, -0.1861, -0.0000]]),\n",
       " 'virials': None,\n",
       " 'stress': None,\n",
       " 'displacement': tensor([[[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]]]),\n",
       " 'll_feats': tensor([[ 0.1767,  0.1193, -0.4057,  ...,  0.0266,  0.0101,  0.0175],\n",
       "         [ 0.1767,  0.1193, -0.4057,  ...,  0.0266,  0.0101,  0.0175]],\n",
       "        grad_fn=<DivBackward0>),\n",
       " 'uncertainty': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llpr_model = modules.models.LLPredRigidityMACE(model_compiled, ll_feat_format=\"avg_over_atom\")\n",
    "llpr_model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__torch__.mace.modules.blocks.LinearReadoutBlock object at 0x29817c8d0>\n",
      "<__torch__.mace.modules.blocks.___torch_mangle_203.LinearReadoutBlock object at 0x29817e740>\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/Users/sanggyu/Research/mace/mace/modules/models.py\", line 418, in forward\n                node_energies = node_energies.squeeze(-1)  # [n_nodes, ]\n            else:\n                raise TypeError(\"Unknown readout for LLPR!\")\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    \n            energy = scatter_sum(\nbuiltins.TypeError: Unknown readout for LLPR!\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m llpr_compiled \u001b[38;5;241m=\u001b[39m jit\u001b[38;5;241m.\u001b[39mcompile(llpr_model)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mllpr_compiled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mace/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mace/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/Users/sanggyu/Research/mace/mace/modules/models.py\", line 418, in forward\n                node_energies = node_energies.squeeze(-1)  # [n_nodes, ]\n            else:\n                raise TypeError(\"Unknown readout for LLPR!\")\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n    \n            energy = scatter_sum(\nbuiltins.TypeError: Unknown readout for LLPR!\n"
     ]
    }
   ],
   "source": [
    "llpr_compiled = jit.compile(llpr_model)\n",
    "llpr_compiled(batch.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for name, f in llpr_compiled.orig_model.readouts.named_children():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
